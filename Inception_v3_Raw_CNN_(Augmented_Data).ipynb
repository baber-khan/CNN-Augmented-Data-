{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception_v3 - Raw - CNN (Augmented Data).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baber-khan/CNN-Augmented-Data-/blob/master/Inception_v3_Raw_CNN_(Augmented_Data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz_tma7gAlsn",
        "colab_type": "code",
        "outputId": "2041ff6c-d13c-4dbe-880f-4d346a4a8bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/Drive\n",
            "Drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RICAeU1MAsVT",
        "colab_type": "code",
        "outputId": "196bff19-4bf6-421e-bea2-ad4424a82c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "\n",
        "os.chdir('/content/Drive/My Drive/Colab Notebooks/Augmented1')\n",
        "\n",
        "!ls  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test  Training\tValidation  vgg16_2.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfJp_AX0AvXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.applications import inception_v3\n",
        "\n",
        "#Load the Inception_V3 model\n",
        "conv_base = inception_v3.InceptionV3(weights='imagenet',\n",
        "                                           include_top = False,\n",
        "                                           input_shape=(150,150,3)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNFzRnG6A17D",
        "colab_type": "code",
        "outputId": "456ad700-fbf1-4c02-83b1-534d576fe339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " conv_base.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 74, 74, 32)   864         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 74, 74, 32)   96          conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 74, 74, 32)   0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 72, 72, 32)   9216        activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 72, 72, 32)   96          conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 72, 72, 32)   0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 72, 72, 64)   18432       activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 72, 72, 64)   192         conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 72, 72, 64)   0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 35, 35, 64)   0           activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 35, 35, 80)   5120        max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 35, 35, 80)   240         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 35, 35, 80)   0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 33, 33, 192)  138240      activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 33, 33, 192)  576         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 33, 33, 192)  0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 16, 16, 192)  0           activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 16, 16, 64)   192         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 16, 16, 64)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 16, 16, 48)   9216        max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 16, 16, 96)   55296       activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 16, 16, 48)   144         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 16, 16, 96)   288         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 16, 16, 48)   0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 16, 16, 96)   0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 16, 16, 192)  0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 16, 16, 64)   76800       activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 16, 16, 96)   82944       activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 16, 16, 32)   6144        average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 16, 16, 64)   192         conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 16, 16, 64)   192         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 16, 16, 96)   288         conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 16, 16, 32)   96          conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 16, 16, 64)   0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 16, 16, 64)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 16, 16, 96)   0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 16, 16, 32)   0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_194[0][0]             \n",
            "                                                                 activation_196[0][0]             \n",
            "                                                                 activation_199[0][0]             \n",
            "                                                                 activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 16, 16, 64)   192         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 16, 16, 64)   0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 16, 16, 96)   55296       activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 16, 16, 48)   144         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 16, 16, 96)   288         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 16, 16, 48)   0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 16, 16, 96)   0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_20 (AveragePo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 16, 16, 64)   76800       activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 16, 16, 96)   82944       activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 16, 16, 64)   16384       average_pooling2d_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 16, 16, 64)   192         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 16, 16, 64)   192         conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 16, 16, 96)   288         conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 16, 16, 64)   192         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 16, 16, 64)   0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 16, 16, 64)   0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 16, 16, 96)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 16, 16, 64)   0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_201[0][0]             \n",
            "                                                                 activation_203[0][0]             \n",
            "                                                                 activation_206[0][0]             \n",
            "                                                                 activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 16, 16, 64)   192         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 16, 16, 64)   0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 16, 16, 96)   55296       activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 16, 16, 48)   144         conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 16, 16, 96)   288         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 16, 16, 48)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 16, 16, 96)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_21 (AveragePo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 16, 16, 64)   76800       activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 16, 16, 96)   82944       activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 16, 16, 64)   18432       average_pooling2d_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 16, 16, 64)   192         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 16, 16, 64)   192         conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 16, 16, 96)   288         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 16, 16, 64)   192         conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 16, 16, 64)   0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 16, 16, 64)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 16, 16, 96)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 16, 16, 64)   0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_208[0][0]             \n",
            "                                                                 activation_210[0][0]             \n",
            "                                                                 activation_213[0][0]             \n",
            "                                                                 activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 16, 16, 64)   192         conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 16, 16, 64)   0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 16, 16, 96)   55296       activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 16, 16, 96)   288         conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 16, 16, 96)   0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 7, 7, 96)     82944       activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 7, 7, 384)    1152        conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 7, 7, 96)     288         conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 7, 7, 384)    0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 7, 7, 96)     0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_215[0][0]             \n",
            "                                                                 activation_218[0][0]             \n",
            "                                                                 max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 7, 7, 128)    384         conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 7, 7, 128)    0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 7, 7, 128)    114688      activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 7, 7, 128)    384         conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 7, 7, 128)    0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 7, 7, 128)    114688      activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 7, 7, 128)    384         conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 7, 7, 128)    384         conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 7, 7, 128)    0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 7, 7, 128)    0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 7, 7, 128)    114688      activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 7, 7, 128)    114688      activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 7, 7, 128)    384         conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 7, 7, 128)    384         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 7, 7, 128)    0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 7, 7, 128)    0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_22 (AveragePo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 7, 7, 192)    172032      activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 7, 7, 192)    172032      activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 7, 7, 192)    576         conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 7, 7, 192)    576         conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 7, 7, 192)    576         conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 7, 7, 192)    576         conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 7, 7, 192)    0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 7, 7, 192)    0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 7, 7, 192)    0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 7, 7, 192)    0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_219[0][0]             \n",
            "                                                                 activation_222[0][0]             \n",
            "                                                                 activation_227[0][0]             \n",
            "                                                                 activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 7, 7, 160)    480         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 7, 7, 160)    0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 7, 7, 160)    179200      activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 7, 7, 160)    480         conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 7, 7, 160)    0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 7, 7, 160)    179200      activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 7, 7, 160)    480         conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 7, 7, 160)    480         conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 7, 7, 160)    0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 7, 7, 160)    0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 7, 7, 160)    179200      activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 7, 7, 160)    179200      activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 7, 7, 160)    480         conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 7, 7, 160)    480         conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 7, 7, 160)    0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 7, 7, 160)    0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_23 (AveragePo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 7, 7, 192)    215040      activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 7, 7, 192)    215040      activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 7, 7, 192)    576         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 7, 7, 192)    576         conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 7, 7, 192)    576         conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 7, 7, 192)    576         conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 7, 7, 192)    0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 7, 7, 192)    0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 7, 7, 192)    0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 7, 7, 192)    0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_229[0][0]             \n",
            "                                                                 activation_232[0][0]             \n",
            "                                                                 activation_237[0][0]             \n",
            "                                                                 activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 7, 7, 160)    480         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 7, 7, 160)    0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 7, 7, 160)    179200      activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 7, 7, 160)    480         conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 7, 7, 160)    0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 7, 7, 160)    179200      activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 7, 7, 160)    480         conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 7, 7, 160)    480         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 7, 7, 160)    0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 7, 7, 160)    0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 7, 7, 160)    179200      activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 7, 7, 160)    179200      activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 7, 7, 160)    480         conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 7, 7, 160)    480         conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 7, 7, 160)    0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 7, 7, 160)    0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_24 (AveragePo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 7, 7, 192)    215040      activation_241[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 7, 7, 192)    215040      activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 7, 7, 192)    576         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 7, 7, 192)    576         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 7, 7, 192)    576         conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 7, 7, 192)    576         conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 7, 7, 192)    0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 7, 7, 192)    0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 7, 7, 192)    0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 7, 7, 192)    0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_239[0][0]             \n",
            "                                                                 activation_242[0][0]             \n",
            "                                                                 activation_247[0][0]             \n",
            "                                                                 activation_248[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 7, 7, 192)    576         conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 7, 7, 192)    0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 7, 7, 192)    258048      activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 7, 7, 192)    576         conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 7, 7, 192)    0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 7, 7, 192)    258048      activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 7, 7, 192)    576         conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 7, 7, 192)    576         conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 7, 7, 192)    0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 7, 7, 192)    0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 7, 7, 192)    258048      activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 7, 7, 192)    258048      activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 7, 7, 192)    576         conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 7, 7, 192)    576         conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 7, 7, 192)    0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 7, 7, 192)    0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_25 (AveragePo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 7, 7, 192)    258048      activation_251[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 7, 7, 192)    258048      activation_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 7, 7, 192)    576         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 7, 7, 192)    576         conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_257 (BatchN (None, 7, 7, 192)    576         conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_258 (BatchN (None, 7, 7, 192)    576         conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 7, 7, 192)    0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 7, 7, 192)    0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 7, 7, 192)    0           batch_normalization_257[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 7, 7, 192)    0           batch_normalization_258[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_249[0][0]             \n",
            "                                                                 activation_252[0][0]             \n",
            "                                                                 activation_257[0][0]             \n",
            "                                                                 activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_261 (BatchN (None, 7, 7, 192)    576         conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 7, 7, 192)    0           batch_normalization_261[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 7, 7, 192)    258048      activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 7, 7, 192)    576         conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 7, 7, 192)    0           batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 7, 7, 192)    258048      activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_259 (BatchN (None, 7, 7, 192)    576         conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 7, 7, 192)    576         conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 7, 7, 192)    0           batch_normalization_259[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 7, 7, 192)    0           batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 3, 3, 320)    552960      activation_259[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 3, 3, 192)    331776      activation_263[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_260 (BatchN (None, 3, 3, 320)    960         conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 3, 3, 192)    576         conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 3, 3, 320)    0           batch_normalization_260[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 3, 3, 192)    0           batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_260[0][0]             \n",
            "                                                                 activation_264[0][0]             \n",
            "                                                                 max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_269 (Conv2D)             (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 3, 3, 448)    1344        conv2d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 3, 3, 448)    0           batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_270 (Conv2D)             (None, 3, 3, 384)    1548288     activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 3, 3, 384)    1152        conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_270 (BatchN (None, 3, 3, 384)    1152        conv2d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 3, 3, 384)    0           batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 3, 3, 384)    0           batch_normalization_270[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 3, 3, 384)    442368      activation_266[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_268 (Conv2D)             (None, 3, 3, 384)    442368      activation_266[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 3, 3, 384)    442368      activation_270[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 3, 3, 384)    442368      activation_270[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_26 (AveragePo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 3, 3, 384)    1152        conv2d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 3, 3, 384)    1152        conv2d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_271 (BatchN (None, 3, 3, 384)    1152        conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_272 (BatchN (None, 3, 3, 384)    1152        conv2d_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 3, 3, 192)    245760      average_pooling2d_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 3, 3, 320)    960         conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 3, 3, 384)    0           batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 3, 3, 384)    0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 3, 3, 384)    0           batch_normalization_271[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 3, 3, 384)    0           batch_normalization_272[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_273 (BatchN (None, 3, 3, 192)    576         conv2d_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 3, 3, 320)    0           batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_267[0][0]             \n",
            "                                                                 activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 3, 3, 768)    0           activation_271[0][0]             \n",
            "                                                                 activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 3, 3, 192)    0           batch_normalization_273[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_265[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_5[0][0]              \n",
            "                                                                 activation_273[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_278 (BatchN (None, 3, 3, 448)    1344        conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 3, 3, 448)    0           batch_normalization_278[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 3, 3, 384)    1548288     activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_275 (BatchN (None, 3, 3, 384)    1152        conv2d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_279 (BatchN (None, 3, 3, 384)    1152        conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 3, 3, 384)    0           batch_normalization_275[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 3, 3, 384)    0           batch_normalization_279[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 3, 3, 384)    442368      activation_275[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 3, 3, 384)    442368      activation_275[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 3, 3, 384)    442368      activation_279[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 3, 3, 384)    442368      activation_279[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_27 (AveragePo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_276 (BatchN (None, 3, 3, 384)    1152        conv2d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_277 (BatchN (None, 3, 3, 384)    1152        conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_280 (BatchN (None, 3, 3, 384)    1152        conv2d_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_281 (BatchN (None, 3, 3, 384)    1152        conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_282 (Conv2D)             (None, 3, 3, 192)    393216      average_pooling2d_27[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_274 (BatchN (None, 3, 3, 320)    960         conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 3, 3, 384)    0           batch_normalization_276[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 3, 3, 384)    0           batch_normalization_277[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 3, 3, 384)    0           batch_normalization_280[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 3, 3, 384)    0           batch_normalization_281[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_282 (BatchN (None, 3, 3, 192)    576         conv2d_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 3, 3, 320)    0           batch_normalization_274[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_276[0][0]             \n",
            "                                                                 activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 3, 3, 768)    0           activation_280[0][0]             \n",
            "                                                                 activation_281[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_282 (Activation)     (None, 3, 3, 192)    0           batch_normalization_282[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_274[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_6[0][0]              \n",
            "                                                                 activation_282[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-miyhY8A558",
        "colab_type": "code",
        "outputId": "bf0eb2f8-256a-4a01-8158-7319b644fa21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir ='/content/Drive/My Drive/Colab Notebooks/Augmented1'\n",
        "train_dir = os.path.join(base_dir, 'Training')\n",
        "validation_dir = os.path.join(base_dir, 'Validation')\n",
        "test_dir = os.path.join(base_dir, 'Test')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 10\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 3, 3, 2048))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "    i=0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)\n",
        "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            break\n",
        "    return features, labels\n",
        "train_features, train_labels = extract_features(train_dir, 2597)\n",
        "validation_features, validation_labels = extract_features(validation_dir, 1558)\n",
        "test_features, test_labels = extract_features(test_dir, 1045)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2597 images belonging to 2 classes.\n",
            "Found 1558 images belonging to 2 classes.\n",
            "Found 1045 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxVqY_cGGgFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = np.reshape(train_features, (2597, 3*3* 2048))\n",
        "validation_features = np.reshape(validation_features, (1558, 3*3* 2048))\n",
        "test_features = np.reshape(test_features, (1045, 3*3* 2048))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp2-qEGYBBag",
        "colab_type": "code",
        "outputId": "a9558cbf-bf4d-495f-a123-7451c42f35c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "import keras\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation='relu', input_dim= 3*3* 2048))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_features, train_labels,\n",
        "epochs=100,\n",
        "batch_size=20,\n",
        "validation_data=(validation_features, validation_labels)\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2597 samples, validate on 1558 samples\n",
            "Epoch 1/100\n",
            "2597/2597 [==============================] - 6s 2ms/step - loss: 0.7658 - acc: 0.5849 - val_loss: 0.6019 - val_acc: 0.6861\n",
            "Epoch 2/100\n",
            "2597/2597 [==============================] - 1s 439us/step - loss: 0.5686 - acc: 0.7074 - val_loss: 0.5483 - val_acc: 0.7478\n",
            "Epoch 3/100\n",
            "2597/2597 [==============================] - 1s 427us/step - loss: 0.4648 - acc: 0.7821 - val_loss: 0.4937 - val_acc: 0.7606\n",
            "Epoch 4/100\n",
            "2597/2597 [==============================] - 1s 436us/step - loss: 0.3986 - acc: 0.8333 - val_loss: 0.4430 - val_acc: 0.8087\n",
            "Epoch 5/100\n",
            "2597/2597 [==============================] - 1s 442us/step - loss: 0.3375 - acc: 0.8625 - val_loss: 0.4866 - val_acc: 0.7426\n",
            "Epoch 6/100\n",
            "2597/2597 [==============================] - 1s 427us/step - loss: 0.2870 - acc: 0.8945 - val_loss: 0.3849 - val_acc: 0.8376\n",
            "Epoch 7/100\n",
            "2597/2597 [==============================] - 1s 421us/step - loss: 0.2458 - acc: 0.9211 - val_loss: 0.3629 - val_acc: 0.8511\n",
            "Epoch 8/100\n",
            "2597/2597 [==============================] - 1s 427us/step - loss: 0.2162 - acc: 0.9342 - val_loss: 0.3501 - val_acc: 0.8601\n",
            "Epoch 9/100\n",
            "2597/2597 [==============================] - 1s 436us/step - loss: 0.1848 - acc: 0.9519 - val_loss: 0.3404 - val_acc: 0.8524\n",
            "Epoch 10/100\n",
            "2597/2597 [==============================] - 1s 444us/step - loss: 0.1562 - acc: 0.9603 - val_loss: 0.3494 - val_acc: 0.8504\n",
            "Epoch 11/100\n",
            "2597/2597 [==============================] - 1s 431us/step - loss: 0.1347 - acc: 0.9669 - val_loss: 0.3124 - val_acc: 0.8678\n",
            "Epoch 12/100\n",
            "2597/2597 [==============================] - 1s 443us/step - loss: 0.1259 - acc: 0.9696 - val_loss: 0.2999 - val_acc: 0.8729\n",
            "Epoch 13/100\n",
            "2597/2597 [==============================] - 1s 439us/step - loss: 0.1104 - acc: 0.9773 - val_loss: 0.3210 - val_acc: 0.8607\n",
            "Epoch 14/100\n",
            "2597/2597 [==============================] - 1s 435us/step - loss: 0.0936 - acc: 0.9827 - val_loss: 0.2874 - val_acc: 0.8787\n",
            "Epoch 15/100\n",
            "2597/2597 [==============================] - 1s 435us/step - loss: 0.0772 - acc: 0.9896 - val_loss: 0.3322 - val_acc: 0.8537\n",
            "Epoch 16/100\n",
            "2597/2597 [==============================] - 1s 432us/step - loss: 0.0726 - acc: 0.9896 - val_loss: 0.2744 - val_acc: 0.8870\n",
            "Epoch 17/100\n",
            "2597/2597 [==============================] - 1s 440us/step - loss: 0.0656 - acc: 0.9908 - val_loss: 0.2652 - val_acc: 0.8986\n",
            "Epoch 18/100\n",
            "2597/2597 [==============================] - 1s 440us/step - loss: 0.0561 - acc: 0.9931 - val_loss: 0.2694 - val_acc: 0.8973\n",
            "Epoch 19/100\n",
            "2597/2597 [==============================] - 1s 431us/step - loss: 0.0511 - acc: 0.9931 - val_loss: 0.2645 - val_acc: 0.8999\n",
            "Epoch 20/100\n",
            "2597/2597 [==============================] - 1s 427us/step - loss: 0.0439 - acc: 0.9946 - val_loss: 0.2780 - val_acc: 0.8909\n",
            "Epoch 21/100\n",
            "2597/2597 [==============================] - 1s 443us/step - loss: 0.0377 - acc: 0.9973 - val_loss: 0.2950 - val_acc: 0.8780\n",
            "Epoch 22/100\n",
            "2597/2597 [==============================] - 1s 432us/step - loss: 0.0367 - acc: 0.9958 - val_loss: 0.2660 - val_acc: 0.8954\n",
            "Epoch 23/100\n",
            "2597/2597 [==============================] - 1s 432us/step - loss: 0.0301 - acc: 0.9981 - val_loss: 0.2826 - val_acc: 0.8896\n",
            "Epoch 24/100\n",
            "2597/2597 [==============================] - 1s 433us/step - loss: 0.0265 - acc: 0.9985 - val_loss: 0.2626 - val_acc: 0.8941\n",
            "Epoch 25/100\n",
            "2597/2597 [==============================] - 1s 437us/step - loss: 0.0252 - acc: 0.9977 - val_loss: 0.2808 - val_acc: 0.8909\n",
            "Epoch 26/100\n",
            "2597/2597 [==============================] - 1s 445us/step - loss: 0.0206 - acc: 0.9981 - val_loss: 0.2460 - val_acc: 0.9076\n",
            "Epoch 27/100\n",
            "2597/2597 [==============================] - 1s 453us/step - loss: 0.0195 - acc: 0.9992 - val_loss: 0.2557 - val_acc: 0.9050\n",
            "Epoch 28/100\n",
            "2597/2597 [==============================] - 1s 467us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.2588 - val_acc: 0.9076\n",
            "Epoch 29/100\n",
            "2597/2597 [==============================] - 1s 462us/step - loss: 0.0159 - acc: 0.9988 - val_loss: 0.3009 - val_acc: 0.8915\n",
            "Epoch 30/100\n",
            "2597/2597 [==============================] - 1s 464us/step - loss: 0.0137 - acc: 0.9996 - val_loss: 0.2717 - val_acc: 0.8947\n",
            "Epoch 31/100\n",
            "2597/2597 [==============================] - 1s 454us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.2791 - val_acc: 0.8979\n",
            "Epoch 32/100\n",
            "2597/2597 [==============================] - 1s 457us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.2585 - val_acc: 0.9056\n",
            "Epoch 33/100\n",
            "2597/2597 [==============================] - 1s 447us/step - loss: 0.0101 - acc: 0.9996 - val_loss: 0.3022 - val_acc: 0.8928\n",
            "Epoch 34/100\n",
            "2597/2597 [==============================] - 1s 448us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.2727 - val_acc: 0.9024\n",
            "Epoch 35/100\n",
            "2597/2597 [==============================] - 1s 459us/step - loss: 0.0080 - acc: 0.9996 - val_loss: 0.2707 - val_acc: 0.9037\n",
            "Epoch 36/100\n",
            "2597/2597 [==============================] - 1s 457us/step - loss: 0.0080 - acc: 0.9985 - val_loss: 0.2656 - val_acc: 0.9056\n",
            "Epoch 37/100\n",
            "2597/2597 [==============================] - 1s 455us/step - loss: 0.0069 - acc: 0.9996 - val_loss: 0.2654 - val_acc: 0.9031\n",
            "Epoch 38/100\n",
            "2597/2597 [==============================] - 1s 462us/step - loss: 0.0062 - acc: 0.9992 - val_loss: 0.2689 - val_acc: 0.9127\n",
            "Epoch 39/100\n",
            "2597/2597 [==============================] - 1s 441us/step - loss: 0.0061 - acc: 0.9996 - val_loss: 0.2636 - val_acc: 0.9063\n",
            "Epoch 40/100\n",
            "2597/2597 [==============================] - 1s 436us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.2855 - val_acc: 0.9063\n",
            "Epoch 41/100\n",
            "2597/2597 [==============================] - 1s 447us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.2922 - val_acc: 0.9031\n",
            "Epoch 42/100\n",
            "2597/2597 [==============================] - 1s 445us/step - loss: 0.0040 - acc: 0.9996 - val_loss: 0.2770 - val_acc: 0.9018\n",
            "Epoch 43/100\n",
            "2597/2597 [==============================] - 1s 443us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.2874 - val_acc: 0.9101\n",
            "Epoch 44/100\n",
            "2597/2597 [==============================] - 1s 443us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.2847 - val_acc: 0.9095\n",
            "Epoch 45/100\n",
            "2597/2597 [==============================] - 1s 446us/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2882 - val_acc: 0.9044\n",
            "Epoch 46/100\n",
            "2597/2597 [==============================] - 1s 439us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.2986 - val_acc: 0.9050\n",
            "Epoch 47/100\n",
            "2597/2597 [==============================] - 1s 456us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.3181 - val_acc: 0.9031\n",
            "Epoch 48/100\n",
            "2597/2597 [==============================] - 1s 449us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3150 - val_acc: 0.9024\n",
            "Epoch 49/100\n",
            "2597/2597 [==============================] - 1s 432us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.3070 - val_acc: 0.9056\n",
            "Epoch 50/100\n",
            "2597/2597 [==============================] - 1s 445us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.2951 - val_acc: 0.9114\n",
            "Epoch 51/100\n",
            "2597/2597 [==============================] - 1s 451us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3112 - val_acc: 0.9056\n",
            "Epoch 52/100\n",
            "2597/2597 [==============================] - 1s 452us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3020 - val_acc: 0.9108\n",
            "Epoch 53/100\n",
            "2597/2597 [==============================] - 1s 447us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3115 - val_acc: 0.9069\n",
            "Epoch 54/100\n",
            "2597/2597 [==============================] - 1s 451us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3172 - val_acc: 0.9108\n",
            "Epoch 55/100\n",
            "2597/2597 [==============================] - 1s 445us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3206 - val_acc: 0.9076\n",
            "Epoch 56/100\n",
            "2597/2597 [==============================] - 1s 458us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3208 - val_acc: 0.9069\n",
            "Epoch 57/100\n",
            "2597/2597 [==============================] - 1s 444us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.3437 - val_acc: 0.9063\n",
            "Epoch 58/100\n",
            "2597/2597 [==============================] - 1s 458us/step - loss: 9.2137e-04 - acc: 1.0000 - val_loss: 0.3283 - val_acc: 0.9095\n",
            "Epoch 59/100\n",
            "2597/2597 [==============================] - 1s 439us/step - loss: 6.8490e-04 - acc: 1.0000 - val_loss: 0.3572 - val_acc: 0.9044\n",
            "Epoch 60/100\n",
            "2597/2597 [==============================] - 1s 440us/step - loss: 5.4709e-04 - acc: 1.0000 - val_loss: 0.3271 - val_acc: 0.9134\n",
            "Epoch 61/100\n",
            "2597/2597 [==============================] - 1s 431us/step - loss: 4.7476e-04 - acc: 1.0000 - val_loss: 0.3373 - val_acc: 0.9114\n",
            "Epoch 62/100\n",
            "2597/2597 [==============================] - 1s 445us/step - loss: 5.1407e-04 - acc: 1.0000 - val_loss: 0.3433 - val_acc: 0.9121\n",
            "Epoch 63/100\n",
            "2597/2597 [==============================] - 1s 429us/step - loss: 4.9177e-04 - acc: 1.0000 - val_loss: 0.3488 - val_acc: 0.9108\n",
            "Epoch 64/100\n",
            "2597/2597 [==============================] - 1s 451us/step - loss: 4.3033e-04 - acc: 1.0000 - val_loss: 0.3572 - val_acc: 0.9101\n",
            "Epoch 65/100\n",
            "2597/2597 [==============================] - 1s 444us/step - loss: 7.0145e-04 - acc: 1.0000 - val_loss: 0.3406 - val_acc: 0.9159\n",
            "Epoch 66/100\n",
            "2597/2597 [==============================] - 1s 443us/step - loss: 3.5992e-04 - acc: 1.0000 - val_loss: 0.3441 - val_acc: 0.9146\n",
            "Epoch 67/100\n",
            "2597/2597 [==============================] - 1s 445us/step - loss: 3.1701e-04 - acc: 1.0000 - val_loss: 0.3810 - val_acc: 0.9089\n",
            "Epoch 68/100\n",
            "2597/2597 [==============================] - 1s 448us/step - loss: 3.1287e-04 - acc: 1.0000 - val_loss: 0.3590 - val_acc: 0.9140\n",
            "Epoch 69/100\n",
            "2597/2597 [==============================] - 1s 441us/step - loss: 3.1798e-04 - acc: 1.0000 - val_loss: 0.3636 - val_acc: 0.9134\n",
            "Epoch 70/100\n",
            "2597/2597 [==============================] - 1s 439us/step - loss: 3.3441e-04 - acc: 1.0000 - val_loss: 0.3576 - val_acc: 0.9146\n",
            "Epoch 71/100\n",
            "2597/2597 [==============================] - 1s 435us/step - loss: 2.7296e-04 - acc: 1.0000 - val_loss: 0.3600 - val_acc: 0.9127\n",
            "Epoch 72/100\n",
            "2597/2597 [==============================] - 1s 451us/step - loss: 2.0504e-04 - acc: 1.0000 - val_loss: 0.3843 - val_acc: 0.9089\n",
            "Epoch 73/100\n",
            "2597/2597 [==============================] - 1s 451us/step - loss: 2.6367e-04 - acc: 1.0000 - val_loss: 0.3733 - val_acc: 0.9146\n",
            "Epoch 74/100\n",
            "2597/2597 [==============================] - 1s 432us/step - loss: 2.1156e-04 - acc: 1.0000 - val_loss: 0.4091 - val_acc: 0.8986\n",
            "Epoch 75/100\n",
            "2597/2597 [==============================] - 1s 442us/step - loss: 1.8537e-04 - acc: 1.0000 - val_loss: 0.3856 - val_acc: 0.9095\n",
            "Epoch 76/100\n",
            "2597/2597 [==============================] - 1s 427us/step - loss: 1.9059e-04 - acc: 1.0000 - val_loss: 0.3841 - val_acc: 0.9095\n",
            "Epoch 77/100\n",
            "2597/2597 [==============================] - 1s 440us/step - loss: 1.6000e-04 - acc: 1.0000 - val_loss: 0.3771 - val_acc: 0.9127\n",
            "Epoch 78/100\n",
            "2597/2597 [==============================] - 1s 430us/step - loss: 1.6088e-04 - acc: 1.0000 - val_loss: 0.3920 - val_acc: 0.9108\n",
            "Epoch 79/100\n",
            "2597/2597 [==============================] - 1s 444us/step - loss: 2.8137e-04 - acc: 1.0000 - val_loss: 0.3763 - val_acc: 0.9172\n",
            "Epoch 80/100\n",
            "2597/2597 [==============================] - 1s 439us/step - loss: 1.5787e-04 - acc: 1.0000 - val_loss: 0.3852 - val_acc: 0.9172\n",
            "Epoch 81/100\n",
            "2597/2597 [==============================] - 1s 436us/step - loss: 1.9381e-04 - acc: 1.0000 - val_loss: 0.4176 - val_acc: 0.9101\n",
            "Epoch 82/100\n",
            "2597/2597 [==============================] - 1s 429us/step - loss: 8.1579e-05 - acc: 1.0000 - val_loss: 0.4152 - val_acc: 0.9134\n",
            "Epoch 83/100\n",
            "2597/2597 [==============================] - 1s 441us/step - loss: 9.9849e-05 - acc: 1.0000 - val_loss: 0.3919 - val_acc: 0.9153\n",
            "Epoch 84/100\n",
            "2597/2597 [==============================] - 1s 446us/step - loss: 7.6287e-05 - acc: 1.0000 - val_loss: 0.4079 - val_acc: 0.9146\n",
            "Epoch 85/100\n",
            "2597/2597 [==============================] - 1s 430us/step - loss: 6.5398e-05 - acc: 1.0000 - val_loss: 0.4199 - val_acc: 0.9095\n",
            "Epoch 86/100\n",
            "2597/2597 [==============================] - 1s 448us/step - loss: 8.8100e-05 - acc: 1.0000 - val_loss: 0.4223 - val_acc: 0.9108\n",
            "Epoch 87/100\n",
            "2597/2597 [==============================] - 1s 431us/step - loss: 1.1429e-04 - acc: 1.0000 - val_loss: 0.4467 - val_acc: 0.9082\n",
            "Epoch 88/100\n",
            "2597/2597 [==============================] - 1s 428us/step - loss: 5.1924e-05 - acc: 1.0000 - val_loss: 0.4381 - val_acc: 0.9095\n",
            "Epoch 89/100\n",
            "2597/2597 [==============================] - 1s 444us/step - loss: 5.3549e-05 - acc: 1.0000 - val_loss: 0.4214 - val_acc: 0.9159\n",
            "Epoch 90/100\n",
            "2597/2597 [==============================] - 1s 449us/step - loss: 2.5960e-04 - acc: 1.0000 - val_loss: 0.4357 - val_acc: 0.9108\n",
            "Epoch 91/100\n",
            "2597/2597 [==============================] - 1s 445us/step - loss: 4.1305e-05 - acc: 1.0000 - val_loss: 0.4444 - val_acc: 0.9076\n",
            "Epoch 92/100\n",
            "2597/2597 [==============================] - 1s 436us/step - loss: 8.2417e-05 - acc: 1.0000 - val_loss: 0.4505 - val_acc: 0.9076\n",
            "Epoch 93/100\n",
            "2597/2597 [==============================] - 1s 442us/step - loss: 7.5353e-05 - acc: 1.0000 - val_loss: 0.4411 - val_acc: 0.9140\n",
            "Epoch 94/100\n",
            "2597/2597 [==============================] - 1s 454us/step - loss: 2.4977e-04 - acc: 1.0000 - val_loss: 0.4335 - val_acc: 0.9121\n",
            "Epoch 95/100\n",
            "2597/2597 [==============================] - 1s 435us/step - loss: 3.2684e-05 - acc: 1.0000 - val_loss: 0.4577 - val_acc: 0.9121\n",
            "Epoch 96/100\n",
            "2597/2597 [==============================] - 1s 442us/step - loss: 3.5135e-05 - acc: 1.0000 - val_loss: 0.4573 - val_acc: 0.9121\n",
            "Epoch 97/100\n",
            "2597/2597 [==============================] - 1s 440us/step - loss: 3.0748e-05 - acc: 1.0000 - val_loss: 0.4378 - val_acc: 0.9127\n",
            "Epoch 98/100\n",
            "2597/2597 [==============================] - 1s 430us/step - loss: 4.8390e-05 - acc: 1.0000 - val_loss: 0.4732 - val_acc: 0.9069\n",
            "Epoch 99/100\n",
            "2597/2597 [==============================] - 1s 455us/step - loss: 3.3301e-05 - acc: 1.0000 - val_loss: 0.4781 - val_acc: 0.9076\n",
            "Epoch 100/100\n",
            "2597/2597 [==============================] - 1s 449us/step - loss: 6.3699e-05 - acc: 1.0000 - val_loss: 0.5097 - val_acc: 0.8999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdYGJv7LBJu2",
        "colab_type": "code",
        "outputId": "8a3180fb-4f93-4cea-eb2f-a4ab49b2fcf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjWWMw04sh8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5-3mqokcL3E",
        "colab_type": "code",
        "outputId": "33947069-d25e-41d1-b4cc-be07d10bbb71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 3, 3, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               4718848   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 26,521,889\n",
            "Trainable params: 26,487,457\n",
            "Non-trainable params: 34,432\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r53fwHtdcON9",
        "colab_type": "code",
        "outputId": "dcf12995-6dcc-4e2f-b093-52221067b7fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "  rescale=1./255,\n",
        "  rotation_range=360,\n",
        "  # width_shift_range=0.2,\n",
        "  # height_shift_range=0.2,\n",
        "  # shear_range=0.2,\n",
        "  zoom_range=0.2,\n",
        "  horizontal_flip=True,\n",
        "  fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "  train_dir,\n",
        "  target_size=(150, 150),\n",
        "  batch_size=20,\n",
        "  class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "  validation_dir,\n",
        "  target_size=(150, 150),\n",
        "  batch_size=20,\n",
        "  class_mode='binary')\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "  optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "  metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "  train_generator,\n",
        "  steps_per_epoch=100,\n",
        "  epochs=30,\n",
        "  validation_data=validation_generator,\n",
        "  validation_steps=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2597 images belonging to 2 classes.\n",
            "Found 1558 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "100/100 [==============================] - 56s 556ms/step - loss: 0.7189 - acc: 0.5715 - val_loss: 0.6120 - val_acc: 0.6850\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 36s 355ms/step - loss: 0.6413 - acc: 0.6357 - val_loss: 0.5701 - val_acc: 0.7200\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 0.5686 - acc: 0.7175 - val_loss: 0.6199 - val_acc: 0.7100\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 37s 372ms/step - loss: 0.5343 - acc: 0.7336 - val_loss: 0.4458 - val_acc: 0.7700\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 0.4532 - acc: 0.7870 - val_loss: 0.4090 - val_acc: 0.8150\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 38s 378ms/step - loss: 0.4100 - acc: 0.8056 - val_loss: 0.3878 - val_acc: 0.8050\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 37s 373ms/step - loss: 0.3610 - acc: 0.8405 - val_loss: 0.3635 - val_acc: 0.8250\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 38s 378ms/step - loss: 0.2899 - acc: 0.8745 - val_loss: 0.2464 - val_acc: 0.8838\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 0.2507 - acc: 0.8990 - val_loss: 0.1837 - val_acc: 0.9050\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 38s 378ms/step - loss: 0.2333 - acc: 0.9059 - val_loss: 0.3183 - val_acc: 0.8850\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 37s 373ms/step - loss: 0.1937 - acc: 0.9210 - val_loss: 0.1562 - val_acc: 0.9450\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 37s 373ms/step - loss: 0.1767 - acc: 0.9325 - val_loss: 0.0773 - val_acc: 0.9800\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 37s 373ms/step - loss: 0.1319 - acc: 0.9465 - val_loss: 0.1247 - val_acc: 0.9700\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 38s 383ms/step - loss: 0.1298 - acc: 0.9499 - val_loss: 0.0611 - val_acc: 0.9750\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 38s 377ms/step - loss: 0.1083 - acc: 0.9578 - val_loss: 0.0396 - val_acc: 0.9800\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 38s 380ms/step - loss: 0.1049 - acc: 0.9605 - val_loss: 0.1323 - val_acc: 0.9596\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 37s 372ms/step - loss: 0.0994 - acc: 0.9644 - val_loss: 0.0173 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            " 97/100 [============================>.] - ETA: 1s - loss: 0.0911 - acc: 0.9649"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7--W__dKxgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-KfyWrPOL2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uByUG_dvxK9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_NOF39wOXvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGtsgng_OmgP",
        "colab_type": "code",
        "outputId": "d0ae5728-4f79-4b4b-ce49-6743c1f88cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "    metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 52s 522ms/step - loss: 0.1269 - acc: 0.9445 - val_loss: 0.0686 - val_acc: 0.9700\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 0.0949 - acc: 0.9575 - val_loss: 0.0919 - val_acc: 0.9667\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.1070 - acc: 0.9595 - val_loss: 0.0745 - val_acc: 0.9730\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0920 - acc: 0.9625 - val_loss: 0.0570 - val_acc: 0.9808\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 49s 488ms/step - loss: 0.0923 - acc: 0.9570 - val_loss: 0.0758 - val_acc: 0.9750\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0883 - acc: 0.9645 - val_loss: 0.0696 - val_acc: 0.9778\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0927 - acc: 0.9585 - val_loss: 0.0603 - val_acc: 0.9790\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 49s 485ms/step - loss: 0.0780 - acc: 0.9665 - val_loss: 0.0634 - val_acc: 0.9768\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 49s 488ms/step - loss: 0.0818 - acc: 0.9680 - val_loss: 0.0448 - val_acc: 0.9820\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0793 - acc: 0.9690 - val_loss: 0.0644 - val_acc: 0.9798\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 48s 478ms/step - loss: 0.0753 - acc: 0.9675 - val_loss: 0.0681 - val_acc: 0.9770\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 49s 487ms/step - loss: 0.0884 - acc: 0.9635 - val_loss: 0.0474 - val_acc: 0.9808\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 48s 480ms/step - loss: 0.0666 - acc: 0.9720 - val_loss: 0.0770 - val_acc: 0.9728\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 0.0764 - acc: 0.9660 - val_loss: 0.0460 - val_acc: 0.9850\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0751 - acc: 0.9725 - val_loss: 0.0657 - val_acc: 0.9758\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 48s 475ms/step - loss: 0.0832 - acc: 0.9630 - val_loss: 0.0516 - val_acc: 0.9790\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0759 - acc: 0.9655 - val_loss: 0.0314 - val_acc: 0.9879\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0652 - acc: 0.9745 - val_loss: 0.0747 - val_acc: 0.9710\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 47s 475ms/step - loss: 0.0748 - acc: 0.9690 - val_loss: 0.0581 - val_acc: 0.9748\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 48s 482ms/step - loss: 0.0695 - acc: 0.9715 - val_loss: 0.0400 - val_acc: 0.9880\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 0.0655 - acc: 0.9735 - val_loss: 0.0480 - val_acc: 0.9819\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0772 - acc: 0.9665 - val_loss: 0.0476 - val_acc: 0.9780\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0642 - acc: 0.9695 - val_loss: 0.0384 - val_acc: 0.9879\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 48s 482ms/step - loss: 0.0552 - acc: 0.9770 - val_loss: 0.0401 - val_acc: 0.9819\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 48s 484ms/step - loss: 0.0557 - acc: 0.9780 - val_loss: 0.0351 - val_acc: 0.9870\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0467 - acc: 0.9805 - val_loss: 0.0465 - val_acc: 0.9859\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0701 - acc: 0.9700 - val_loss: 0.0388 - val_acc: 0.9860\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 0.0471 - acc: 0.9825 - val_loss: 0.0492 - val_acc: 0.9859\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0582 - acc: 0.9760 - val_loss: 0.0354 - val_acc: 0.9870\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0648 - acc: 0.9750 - val_loss: 0.0356 - val_acc: 0.9899\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 48s 480ms/step - loss: 0.0610 - acc: 0.9720 - val_loss: 0.0339 - val_acc: 0.9880\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 48s 475ms/step - loss: 0.0621 - acc: 0.9780 - val_loss: 0.0379 - val_acc: 0.9899\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 48s 475ms/step - loss: 0.0569 - acc: 0.9765 - val_loss: 0.0287 - val_acc: 0.9880\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 48s 478ms/step - loss: 0.0569 - acc: 0.9760 - val_loss: 0.0369 - val_acc: 0.9869\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0519 - acc: 0.9810 - val_loss: 0.0395 - val_acc: 0.9879\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 48s 478ms/step - loss: 0.0597 - acc: 0.9785 - val_loss: 0.0395 - val_acc: 0.9840\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 0.0584 - acc: 0.9745 - val_loss: 0.0265 - val_acc: 0.9919\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 48s 478ms/step - loss: 0.0495 - acc: 0.9820 - val_loss: 0.0221 - val_acc: 0.9930\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 47s 475ms/step - loss: 0.0579 - acc: 0.9775 - val_loss: 0.0377 - val_acc: 0.9839\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 0.0590 - acc: 0.9790 - val_loss: 0.0279 - val_acc: 0.9890\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 0.0465 - acc: 0.9775 - val_loss: 0.0426 - val_acc: 0.9859\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - 48s 480ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0307 - val_acc: 0.9880\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - 48s 478ms/step - loss: 0.0581 - acc: 0.9780 - val_loss: 0.0315 - val_acc: 0.9859\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - 48s 482ms/step - loss: 0.0525 - acc: 0.9750 - val_loss: 0.0277 - val_acc: 0.9900\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0561 - acc: 0.9780 - val_loss: 0.0228 - val_acc: 0.9909\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 0.0386 - acc: 0.9850 - val_loss: 0.0351 - val_acc: 0.9859\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 0.0551 - acc: 0.9775 - val_loss: 0.0216 - val_acc: 0.9950\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - 49s 487ms/step - loss: 0.0408 - acc: 0.9815 - val_loss: 0.0307 - val_acc: 0.9829\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - 50s 504ms/step - loss: 0.0512 - acc: 0.9780 - val_loss: 0.0207 - val_acc: 0.9920\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - 49s 485ms/step - loss: 0.0405 - acc: 0.9830 - val_loss: 0.0336 - val_acc: 0.9899\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - 48s 485ms/step - loss: 0.0465 - acc: 0.9800 - val_loss: 0.0463 - val_acc: 0.9870\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0466 - acc: 0.9850 - val_loss: 0.0147 - val_acc: 0.9919\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0398 - acc: 0.9835 - val_loss: 0.0263 - val_acc: 0.9880\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0470 - acc: 0.9795 - val_loss: 0.0592 - val_acc: 0.9768\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 0.0429 - acc: 0.9815 - val_loss: 0.0215 - val_acc: 0.9910\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - 49s 488ms/step - loss: 0.0419 - acc: 0.9875 - val_loss: 0.0169 - val_acc: 0.9940\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0558 - acc: 0.9790 - val_loss: 0.0447 - val_acc: 0.9850\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0393 - acc: 0.9825 - val_loss: 0.0212 - val_acc: 0.9929\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0410 - acc: 0.9805 - val_loss: 0.0335 - val_acc: 0.9849\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0505 - acc: 0.9800 - val_loss: 0.0231 - val_acc: 0.9910\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0390 - acc: 0.9835 - val_loss: 0.0154 - val_acc: 0.9960\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - 48s 480ms/step - loss: 0.0509 - acc: 0.9825 - val_loss: 0.0220 - val_acc: 0.9900\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 0.0395 - acc: 0.9855 - val_loss: 0.0258 - val_acc: 0.9889\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - 49s 492ms/step - loss: 0.0484 - acc: 0.9820 - val_loss: 0.0375 - val_acc: 0.9910\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - 49s 490ms/step - loss: 0.0554 - acc: 0.9855 - val_loss: 0.0160 - val_acc: 0.9940\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 0.0336 - acc: 0.9870 - val_loss: 0.0330 - val_acc: 0.9890\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0471 - acc: 0.9830 - val_loss: 0.0236 - val_acc: 0.9929\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 0.0394 - acc: 0.9830 - val_loss: 0.0235 - val_acc: 0.9950\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - 51s 510ms/step - loss: 0.0410 - acc: 0.9845 - val_loss: 0.0307 - val_acc: 0.9909\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - 49s 487ms/step - loss: 0.0340 - acc: 0.9870 - val_loss: 0.0250 - val_acc: 0.9909\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - 48s 482ms/step - loss: 0.0296 - acc: 0.9910 - val_loss: 0.0260 - val_acc: 0.9940\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - 48s 480ms/step - loss: 0.0373 - acc: 0.9860 - val_loss: 0.0180 - val_acc: 0.9950\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 0.0357 - acc: 0.9850 - val_loss: 0.0183 - val_acc: 0.9920\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 51s 506ms/step - loss: 0.0383 - acc: 0.9865 - val_loss: 0.0187 - val_acc: 0.9919\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0352 - acc: 0.9885 - val_loss: 0.0218 - val_acc: 0.9910\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - 48s 485ms/step - loss: 0.0378 - acc: 0.9880 - val_loss: 0.0331 - val_acc: 0.9879\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 49s 487ms/step - loss: 0.0370 - acc: 0.9885 - val_loss: 0.0246 - val_acc: 0.9930\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 0.0239 - acc: 0.9910 - val_loss: 0.0246 - val_acc: 0.9929\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - 52s 518ms/step - loss: 0.0443 - acc: 0.9860 - val_loss: 0.0124 - val_acc: 0.9950\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 49s 490ms/step - loss: 0.0344 - acc: 0.9880 - val_loss: 0.0265 - val_acc: 0.9940\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 49s 489ms/step - loss: 0.0349 - acc: 0.9860 - val_loss: 0.0125 - val_acc: 0.9940\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 49s 489ms/step - loss: 0.0486 - acc: 0.9825 - val_loss: 0.0165 - val_acc: 0.9940\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 49s 486ms/step - loss: 0.0345 - acc: 0.9880 - val_loss: 0.0151 - val_acc: 0.9929\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 48s 482ms/step - loss: 0.0334 - acc: 0.9885 - val_loss: 0.0300 - val_acc: 0.9900\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - 49s 487ms/step - loss: 0.0406 - acc: 0.9840 - val_loss: 0.0163 - val_acc: 0.9929\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 0.0235 - acc: 0.9915 - val_loss: 0.0086 - val_acc: 0.9960\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 48s 482ms/step - loss: 0.0241 - acc: 0.9905 - val_loss: 0.0173 - val_acc: 0.9940\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 48s 484ms/step - loss: 0.0523 - acc: 0.9830 - val_loss: 0.0206 - val_acc: 0.9930\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 0.0277 - acc: 0.9865 - val_loss: 0.0137 - val_acc: 0.9960\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 0.0295 - acc: 0.9865 - val_loss: 0.0122 - val_acc: 0.9950\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 48s 481ms/step - loss: 0.0407 - acc: 0.9860 - val_loss: 0.0171 - val_acc: 0.9960\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 0.0319 - acc: 0.9880 - val_loss: 0.0225 - val_acc: 0.9919\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0296 - acc: 0.9920 - val_loss: 0.0194 - val_acc: 0.9940\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 49s 489ms/step - loss: 0.0275 - acc: 0.9895 - val_loss: 0.0236 - val_acc: 0.9899\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.0383 - acc: 0.9885 - val_loss: 0.0133 - val_acc: 0.9960\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 0.0235 - acc: 0.9880 - val_loss: 0.0084 - val_acc: 0.9970\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 0.0395 - acc: 0.9870 - val_loss: 0.0150 - val_acc: 0.9930\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 0.0312 - acc: 0.9890 - val_loss: 0.0257 - val_acc: 0.9899\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 0.0289 - acc: 0.9885 - val_loss: 0.0262 - val_acc: 0.9890\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - 48s 475ms/step - loss: 0.0280 - acc: 0.9895 - val_loss: 0.0576 - val_acc: 0.9849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA7GpfhVO41F",
        "colab_type": "code",
        "outputId": "12abf2f3-2c97-42ad-c6e7-e77c66a5a316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "def smooth_curve(points, factor=0.1):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "        smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "plt.plot(epochs,\n",
        "  smooth_curve(acc), 'bo', label='Smoothed training acc')\n",
        "plt.plot(epochs,\n",
        "  smooth_curve(val_acc), 'b', label='Smoothed validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs,\n",
        "  smooth_curve(loss), 'bo', label='Smoothed training loss')\n",
        "plt.plot(epochs,\n",
        "  smooth_curve(val_loss), 'b', label='Smoothed validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eardsd0WpHzA",
        "colab_type": "code",
        "outputId": "76a0c515-e80b-4f84-98fb-888ee2742879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1096 images belonging to 2 classes.\n",
            "test acc: 0.9779999947547913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjxmmPdfyWX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}